{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHGvXG3T5Bv7"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import math\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CzIT_YGC5gx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54292bd5-7c72-4a2f-8591-ff0ed9d5d09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\") #spacy for tokenisation"
      ],
      "metadata": {
        "id": "GL3X5Yz4CqeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary Class\n",
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod #To avoid the tokeniser_eng taking two args\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "  def build_vocabulary(self, sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4 # starts at 4 0123 are pseu\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "        for token in tokenized_text\n",
        "        ]"
      ],
      "metadata": {
        "id": "tex9ast8ECvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Class\n",
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    self.imgs = self.df[\"image\"]\n",
        "    self.captions = self.df[\"caption\"]\n",
        "\n",
        "    self.vocab = Vocabulary(freq_threshold)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_caption += self.vocab.numericalize(caption)\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)"
      ],
      "metadata": {
        "id": "_u52-ahsCe_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Masking of words\n",
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "6e_w-OdUFFaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loader\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=8,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "  dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "  loader = DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=shuffle,\n",
        "      pin_memory=pin_memory,\n",
        "      collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "  return loader, dataset"
      ],
      "metadata": {
        "id": "HyGNlLEOJndN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder class. Used pretrained Inception Model\n",
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train_CNN= False):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    self.train_CNN = train_CNN\n",
        "    self.inception = models.inception_v3(pretrained = True, aux_logits = True)\n",
        "    self.inception.aux_logits = False\n",
        "    self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.inception(images)\n",
        "\n",
        "    for name, param in self.inception.named_parameters():\n",
        "      if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "      else:\n",
        "        param.requires_grad = self.train_CNN\n",
        "    return self.dropout(self.relu(features))\n"
      ],
      "metadata": {
        "id": "GvqCjXrv5kTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder model with attention\n",
        "class DecoderRNNAttn(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNNAttn, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.attention = nn.Linear(hidden_size, hidden_size)\n",
        "        self.attention_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions)\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        attention_weights = self.softmax(self.attention(lstm_out))\n",
        "        attention_applied = attention_weights * lstm_out\n",
        "        attention_combined = torch.cat((lstm_out, attention_applied), dim=2)\n",
        "        output = self.attention_combine(attention_combined)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "7Vcy3yc5umSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN to RNN connect\n",
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoderCNN = EncoderCNN(embed_size)\n",
        "    self.decoderRNN = DecoderRNNAttn(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoderCNN(images)\n",
        "    output = self.decoderRNN(features, captions)\n",
        "    return output\n",
        "\n",
        "def caption_image(self,image, vocabulary, max_len=30): #Word limit set to 30\n",
        "  result_caption = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x = self.encoderCNN(image).unsqueeze(0)\n",
        "    states = None\n",
        "\n",
        "    for _ in range (max_len):\n",
        "      hiddens, states = self.decoderRNNAttn.lstm(x, states)\n",
        "      output = self.decoderRNNAttn.linear(hiddens.squeeze(0))\n",
        "      predicted = output.argmax(1) #output with max prob\n",
        "\n",
        "      result_caption.append(predicted.item())\n",
        "      x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "      if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "        break\n",
        "\n",
        "    return [vocabulary.itos[idx] for idx in result_caption] #convert to word"
      ],
      "metadata": {
        "id": "xrFwEqzGopL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "transform = transforms.Compose(\n",
        "      [\n",
        "          transforms.Resize((356, 356)),\n",
        "          transforms.RandomCrop((299, 299)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "train_loader, dataset = get_loader(\n",
        "      root_folder = \"/content/drive/MyDrive/Image Captioning/flickr8k/images\",\n",
        "      annotation_file = \"/content/drive/MyDrive/Image Captioning/flickr8k/captions.txt\",\n",
        "      transform = transform,\n",
        "      num_workers = 2,\n",
        "  )\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(dataset.vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 10\n",
        "max_len = 30\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "writer = SummaryWriter(\"runs/flickr\")\n",
        "step = 0\n",
        "batch_size = 8\n",
        "\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for idx, (imgs, captions) in enumerate(train_loader):\n",
        "    imgs = imgs.to(device)\n",
        "    captions = captions.to(device)\n",
        "\n",
        "    outputs = model(imgs, captions[:]) #[8,2994]\n",
        "    loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
        "\n",
        "    writer.add_scalar(\"Training loss\", loss.item(), global_step = step)\n",
        "    step += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(loss)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "7InUSX0gMvvK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}