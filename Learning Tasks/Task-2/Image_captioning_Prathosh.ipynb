{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "WSq2MDaUCk45"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "drive_path=\"/content/drive/Mydrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTBka2YcCkL7",
        "outputId": "0507df2f-5ecd-42cf-cd2a-91ffb6083217"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "ZdsZaq8QnRYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the English language model in Spacy\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the vocabulary (number of unique tokens)\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        # Tokenize the input text\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx # Assign the index to the word in the stoi dictionary\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        # Get the number of images in the folder\n",
        "        num_images = len(os.listdir(self.root_dir))\n",
        "\n",
        "\n",
        "\n",
        "        # Read the captions file into a DataFrame\n",
        "        self.df = pd.read_csv(captions_file, delimiter=',', names=['image', 'caption'])\n",
        "\n",
        "        # Filter the DataFrame to include only records with images in the image folder\n",
        "        self.df = self.df[self.df['image'].isin(os.listdir(self.root_dir))]\n",
        "\n",
        "        # Print the number of images in the folder and the DataFrame length\n",
        "        print(\"Number of images in folder:\", num_images)\n",
        "        print(\"Number of records in DataFrame:\", len(self.df))\n",
        "\n",
        "\n",
        "        mismatched_files = set(os.listdir(self.root_dir)) ^ set(self.df['image'])\n",
        "        num_mismatched = len(mismatched_files)\n",
        "\n",
        "        # Remove the mismatched files from the image folder\n",
        "        for file in mismatched_files:\n",
        "            os.remove(os.path.join(self.root_dir, file))\n",
        "\n",
        "        # Filter the DataFrame again to remove the records corresponding to the mismatched files\n",
        "        self.df = self.df[~self.df['image'].isin(mismatched_files)]\n",
        "\n",
        "        self.transform = transform\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        print(\"Number of images in folder:\", num_images)\n",
        "        print(\"Number of records in DataFrame:\", len(self.df))\n",
        "        print(\"Number of mismatched files:\", num_mismatched)\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      caption = self.captions[index]\n",
        "      img_id = self.imgs[index]\n",
        "\n",
        "\n",
        "      img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "      if self.transform is not None:\n",
        "          img = self.transform(img)\n",
        "\n",
        "      numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "      numericalized_caption += self.vocab.numericalize(caption)\n",
        "      numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "      return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "neVdJSW6psF0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
        "    )\n",
        "\n",
        "loader, dataset = get_loader(\"/content/drive/MyDrive/images\", \"/content/drive/MyDrive/captions.txt\", transform=transform  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZBs8wfDCb7X",
        "outputId": "4b60d1f7-92a9-4773-a90c-862c41c0f3f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in folder: 1389\n",
            "Number of records in DataFrame: 6945\n",
            "Number of images in folder: 1389\n",
            "Number of records in DataFrame: 6945\n",
            "Number of mismatched files: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "        self.inception.aux_logits=False\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.times = []\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "\n",
        "        for name, param in self.inception.named_parameters():\n",
        "          if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "              param.requires_grad = True\n",
        "          else:\n",
        "              param.requires_grad = self.train_CNN\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                # Embed the predicted word and pass it as input for the next time step\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "        # Convert the predicted indices to their corresponding words using the vocabulary\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]\n"
      ],
      "metadata": {
        "id": "TiaWjgJ8eaBH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def print_examples(model, device, dataset):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    test_img1 = transform(Image.open(\"/content/drive/MyDrive/test_set/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "    print(\n",
        "        \"Example 1 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img2 = transform(\n",
        "        Image.open(\"/content/drive/MyDrive/test_set/child.jpg\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "    print(\n",
        "        \"Example 2 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img3 = transform(Image.open(\"/content/drive/MyDrive/test_set/bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "    print(\n",
        "        \"Example 3 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DejZu_f9m8NO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def train():\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((356, 356)),\n",
        "            transforms.RandomCrop((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_loader, dataset = get_loader(\n",
        "        root_folder=\"/content/drive/MyDrive/new_images/new_images\",\n",
        "        annotation_file=\"/content/drive/MyDrive/captions.txt\",\n",
        "        transform=transform,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    load_model = False\n",
        "    save_model = False\n",
        "    train_CNN = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    embed_size = 256\n",
        "    hidden_size = 256\n",
        "    vocab_size = len(dataset.vocab)\n",
        "    num_layers = 1\n",
        "    learning_rate = 3e-4\n",
        "    num_epochs = 2\n",
        "\n",
        "\n",
        "\n",
        "    # initialize model, loss etc\n",
        "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print_examples(model, device, dataset)\n",
        "\n",
        "\n",
        "        for idx, (imgs, captions) in tqdm(\n",
        "            enumerate(train_loader), total=len(train_loader), leave=False\n",
        "        ):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            outputs = model(imgs, captions[:-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "ZPVIVaQbDioS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}