{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1iO6-LMQqMrB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH3ct5YyQ3Js",
        "outputId": "7d872e16-8c08-4067-e057-ae5f311545f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6kT3LXGVqfVn"
      },
      "outputs": [],
      "source": [
        "#The double conv Class: Double conv2d each followed by a batchnorm2d and Relu activation.\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels,out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True)\n",
        "    )\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ejmf0fq1sQ6l"
      },
      "outputs": [],
      "source": [
        "#Architecture Class\n",
        "class UNET(nn.Module):\n",
        "  def __init__(self, in_channels = 3, out_channels = 2, features = [64, 128, 256, 512]):\n",
        "    super(UNET, self).__init__()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size = 2, stride = 2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "    \n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    skip_connections = []\n",
        "    \n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    \n",
        "    x = self.bottleneck(x) # bottom layer\n",
        "    skip_connections = skip_connections[::-1]\n",
        "\n",
        "    for idx in range(0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "      concat_skip = torch.cat((skip_connection,x), dim = 1) #concatenating Skip connection layer\n",
        "      x = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "    return self.final_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-f4sWnhJFrv",
        "outputId": "00031fd5-e173-44f6-ac01-c58d27711150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 160, 160])\n"
          ]
        }
      ],
      "source": [
        "#Testing the architecture\n",
        "def test():\n",
        "  x = torch.randn((3,1,160,160))\n",
        "  model = UNET(in_channels = 1, out_channels = 2,)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "21Xsc1qTfDdt"
      },
      "outputs": [],
      "source": [
        "#Dataset\n",
        "class ForestAreaDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform = None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace(\"sat\", \"mask\"))\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype = np.float32) # L is for single channel\n",
        "    image = image/255.0\n",
        "    mask = mask/255.0\n",
        "\n",
        "\n",
        "    if self.transform is not None:\n",
        "      augmentations = self.transform(image = image,mask = mask)\n",
        "      image = augmentations[\"image\"]\n",
        "      mask = augmentations[\"mask\"]\n",
        "    \n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "m4K2l3tEE4Gx"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_EPOCHS = 10\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/Forest area dataset/train_image/\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/Forest area dataset/train_mask/\"\n",
        "TEST_IMG_DIR = \"/content/drive/MyDrive/Forest area dataset/test_image/\"\n",
        "TEST_MASK_DIR = \"/content/drive/MyDrive/Forest area dataset/test_mask/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8GBU3YtCuqbk"
      },
      "outputs": [],
      "source": [
        "# To check accuracy, using every pixel\n",
        "def check_accuracy(loader, model, device = DEVICE):\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device).float() #dtype error without float\n",
        "      y = y.to(device).unsqueeze(1).float() #unsqueeze to get the correct dims\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds > 0.5).float()\n",
        "      num_correct += (preds == y).sum()\n",
        "      num_pixels += torch.numel(preds) \n",
        "\n",
        "  print(\n",
        "      f\"Got {num_correct}/{num_pixels} with % accuracy {(num_correct/num_pixels)*100:.2f} \"\n",
        "  )\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XmRejrL3u1iw"
      },
      "outputs": [],
      "source": [
        "#Train and test loader using DataLoader\n",
        "def get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    TEST_IMG_DIR,\n",
        "    TEST_MASK_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    test_transform,\n",
        "    NUM_WORKERS = 2,\n",
        "    PIN_MEMORY = True\n",
        "):\n",
        "  train_ds = ForestAreaDataset(\n",
        "      image_dir =TRAIN_IMG_DIR,\n",
        "      mask_dir = TRAIN_MASK_DIR,\n",
        "      transform = train_transform\n",
        "  )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = True\n",
        "  )\n",
        "  test_ds = ForestAreaDataset(\n",
        "      image_dir =TEST_IMG_DIR,\n",
        "      mask_dir = TEST_MASK_DIR,\n",
        "      transform = test_transform\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      test_ds,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = False\n",
        "  )\n",
        "  \n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_as_imgs(\n",
        "    loader, model, folder = '/content/drive/MyDrive/Check',device=\"cuda\"\n",
        "):\n",
        "  model.eval()\n",
        "  for idx, (x,y) in enumerate(loader):\n",
        "    x = x.to(device=DEVICE).float()\n",
        "    with torch.no_grad():\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds >0.5).float()\n",
        "    torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")"
      ],
      "metadata": {
        "id": "X4i5DhLeqIg5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUhJGcmQTkfk",
        "outputId": "d17e69e7-f727-4940-d047-b197752f091e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:36<00:00,  2.55it/s, loss=0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 60853000/76873728 with % accuracy 79.16 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.65it/s, loss=0.517]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 61438972/76873728 with % accuracy 79.92 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.65it/s, loss=0.418]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 61814295/76873728 with % accuracy 80.41 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.65it/s, loss=0.429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 61887566/76873728 with % accuracy 80.51 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.65it/s, loss=0.299]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 61288584/76873728 with % accuracy 79.73 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.66it/s, loss=0.343]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 62364893/76873728 with % accuracy 81.13 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.67it/s, loss=0.419]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 62347674/76873728 with % accuracy 81.10 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.65it/s, loss=0.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 62651410/76873728 with % accuracy 81.50 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.66it/s, loss=0.464]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 62332782/76873728 with % accuracy 81.08 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:32<00:00,  2.66it/s, loss=0.376]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 62437900/76873728 with % accuracy 81.22 \n"
          ]
        }
      ],
      "source": [
        "#train loop. tqdm for progress bar\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "    data = data.to(device = DEVICE).float() #dtype error without .float\n",
        "    targets = targets.to(device = DEVICE).float().unsqueeze(1)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "\n",
        "\n",
        "train_transform = A.Compose(\n",
        "      [\n",
        "        A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "        A.Rotate(limit = 30, p = 0.4),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.4),  \n",
        "        ToTensorV2(),\n",
        "      ],\n",
        "    )\n",
        "\n",
        "test_transform = A.Compose(\n",
        "      [\n",
        "          A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "          ToTensorV2(),\n",
        "      ],\n",
        "    )\n",
        "\n",
        "model = UNET(in_channels = 3, out_channels = 1).to(DEVICE)\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "train_loader, test_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_MASK_DIR,\n",
        "      TEST_IMG_DIR,\n",
        "      TEST_MASK_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      test_transform,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY,\n",
        ")\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "  check_accuracy(test_loader, model, device = DEVICE)\n",
        "  model.eval()\n",
        "\n",
        "save_predictions_as_imgs(test_loader, model, folder = '/content/drive/MyDrive/Check',device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlQhhPSZ_UMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}