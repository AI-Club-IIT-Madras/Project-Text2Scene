{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1iO6-LMQqMrB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH3ct5YyQ3Js",
        "outputId": "56b17aa8-5c49-442f-b248-fb080e4c6260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6kT3LXGVqfVn"
      },
      "outputs": [],
      "source": [
        "#The double conv Class: Double conv2d each followed by a batchnorm2d and Relu activation.\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels,out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size = 3, stride = 1, padding = 1, bias = False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace = True)\n",
        "    )\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ejmf0fq1sQ6l"
      },
      "outputs": [],
      "source": [
        "#Architecture Class\n",
        "class UNET(nn.Module):\n",
        "  def __init__(self, in_channels = 3, out_channels = 2, features = [64, 128, 256, 512]):\n",
        "    super(UNET, self).__init__()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size = 2, stride = 2))\n",
        "      self.ups.append(DoubleConv(feature*2, feature))\n",
        "    \n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    skip_connections = []\n",
        "    \n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    \n",
        "    x = self.bottleneck(x) # bottom layer\n",
        "    skip_connections = skip_connections[::-1]\n",
        "\n",
        "    for idx in range(0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "      concat_skip = torch.cat((skip_connection,x), dim = 1) #concatenating Skip connection layer\n",
        "      x = self.ups[idx + 1](concat_skip)\n",
        "\n",
        "    return self.final_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-f4sWnhJFrv",
        "outputId": "a1a8a9a3-2bf8-4aa3-b17a-3b3939abce4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 160, 160])\n"
          ]
        }
      ],
      "source": [
        "#Testing the architecture\n",
        "def test():\n",
        "  x = torch.randn((3,1,160,160))\n",
        "  model = UNET(in_channels = 1, out_channels = 2,)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "21Xsc1qTfDdt"
      },
      "outputs": [],
      "source": [
        "#Dataset\n",
        "class ForestAreaDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform = None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = os.path.join(self.image_dir, self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[index].replace(\"sat\", \"mask\"))\n",
        "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype = np.float32) # L is for single channel\n",
        "    image = np.transpose(image) #Transpose to get the correct dims. Was (256,256,3) to (3,256,256)\n",
        "    mask = np.transpose(mask)\n",
        "    mask[mask == 255.0] = 1.0\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m4K2l3tEE4Gx"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "LEARNING_RATE = 5 * 1e-5\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_EPOCHS = 3\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/Forest area dataset/train_image/\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/Forest area dataset/train_mask/\"\n",
        "TEST_IMG_DIR = \"/content/drive/MyDrive/Forest area dataset/test_image/\"\n",
        "TEST_MASK_DIR = \"/content/drive/MyDrive/Forest area dataset/test_mask/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8GBU3YtCuqbk"
      },
      "outputs": [],
      "source": [
        "# To check accuracy, using every pixel\n",
        "def check_accuracy(loader, model, device = DEVICE):\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device).float() #dtype error without float\n",
        "      y = y.to(device).unsqueeze(1).float() #unsqueeze to get the correct dims\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds > 0.5).float()\n",
        "      num_correct += (preds == y).sum()\n",
        "      num_pixels += torch.numel(preds) \n",
        "\n",
        "  print(\n",
        "      f\"Got {num_correct}/{num_pixels} with % accuracy {(num_correct/num_pixels)*100:.2f} \"\n",
        "  )\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XmRejrL3u1iw"
      },
      "outputs": [],
      "source": [
        "#Train and test loader using DataLoader\n",
        "def get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    TEST_IMG_DIR,\n",
        "    TEST_MASK_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    NUM_WORKERS = 2,\n",
        "    PIN_MEMORY = True\n",
        "):\n",
        "  train_ds = ForestAreaDataset(\n",
        "      image_dir =TRAIN_IMG_DIR,\n",
        "      mask_dir = TRAIN_MASK_DIR,\n",
        "  )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = True\n",
        "  )\n",
        "  test_ds = ForestAreaDataset(\n",
        "      image_dir =TEST_IMG_DIR,\n",
        "      mask_dir = TEST_MASK_DIR,\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      test_ds,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = False\n",
        "  )\n",
        "  \n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUhJGcmQTkfk",
        "outputId": "910aecf5-549f-456e-b144-98eae0426641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:34<00:00,  2.61it/s, loss=-3.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 54956606/76873728 with % accuracy 71.49 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:34<00:00,  2.61it/s, loss=-8.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 55127224/76873728 with % accuracy 71.71 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 246/246 [01:34<00:00,  2.60it/s, loss=-13.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 53000987/76873728 with % accuracy 68.95 \n"
          ]
        }
      ],
      "source": [
        "#train loop. tqdm for progress bar\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "    data = data.to(device = DEVICE).float() #dtype error without .float\n",
        "    targets = targets.to(device = DEVICE).float().unsqueeze(1)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "\n",
        "def main():\n",
        "  train_transform = None\n",
        "  test_transform = None\n",
        "  model = UNET(in_channels = 3, out_channels = 1).to(DEVICE)\n",
        "  loss_fn = nn.BCEWithLogitsLoss() #Is it really sigmoid followed by BCE?\n",
        "  optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "  train_loader, test_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_MASK_DIR,\n",
        "      TEST_IMG_DIR,\n",
        "      TEST_MASK_DIR,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      test_transform,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY,\n",
        "  )\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "    check_accuracy(test_loader, model, device = DEVICE)\n",
        "  return model\n",
        "\n",
        "if __name__ == '__main__':  #Why so slow without this? \n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}